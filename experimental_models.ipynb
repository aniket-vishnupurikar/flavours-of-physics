{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets\n",
    "train_df = pd.read_csv(\"training.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "agree_df = pd.read_csv(\"check_agreement.csv\")\n",
    "corr_df = pd.read_csv(\"check_correlation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "from hep_ml.losses import BinFlatnessLossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 51)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training UGradientBoosting model with train dataset with columns which are in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal']\n",
    "features = list(f for f in train_df.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter...\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt',\n",
       "                                            'DOCAone', 'DOCAtwo', 'DOCAthree',\n",
       "                                            'IP_p0p2', 'IP_p1p2', 'isolationa',\n",
       "                                            'isolationb', 'isolationc',\n",
       "                                            'isolationd', 'isolatione',\n",
       "                                            'isolationf', 'iso', 'CDF1', 'CDF2',\n",
       "                                            'CDF3', 'ISO_SumBDT', 'p0_IsoBDT',\n",
       "                                            'p1_IsoBDT', 'p2_IsoBDT',\n",
       "                                            'p0_track_Chi2Dof',\n",
       "                                            'p1_track_Chi2Dof',\n",
       "                                            'p2_track_Chi2Dof', ...],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df[features + ['mass']], train_df['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check agrrement test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n",
    "    \"\"\"\n",
    "    Compute roc curve\n",
    "\n",
    "    :param data_zero: 0-labeled data\n",
    "    :param data_one:  1-labeled data\n",
    "    :param sample_weights_zero: weights for 0-labeled data\n",
    "    :param sample_weights_one:  weights for 1-labeled data\n",
    "    :return: roc curve\n",
    "    \"\"\"\n",
    "    labels = [0] * len(data_zero) + [1] * len(data_one)\n",
    "    weights = np.concatenate([sample_weights_zero, sample_weights_one])\n",
    "    data_all = np.concatenate([data_zero, data_one])\n",
    "    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n",
    "    return fpr, tpr\n",
    "\n",
    "def compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n",
    "    \"\"\"\n",
    "    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n",
    "\n",
    "    :param data_prediction: array-like, real data predictions\n",
    "    :param mc_prediction: array-like, Monte Carlo data predictions\n",
    "    :param weights_data: array-like, real data weights\n",
    "    :param weights_mc: array-like, Monte Carlo weights\n",
    "    :return: ks value\n",
    "    \"\"\"\n",
    "    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n",
    "    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n",
    "\n",
    "    data_prediction, mc_prediction = np.array(data_prediction), np.array(mc_prediction)\n",
    "    weights_data, weights_mc = np.array(weights_data), np.array(weights_mc)\n",
    "\n",
    "    assert np.all(data_prediction >= 0.) and np.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n",
    "    assert np.all(mc_prediction >= 0.) and np.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n",
    "\n",
    "    weights_data /= np.sum(weights_data)\n",
    "    weights_mc /= np.sum(weights_mc)\n",
    "\n",
    "    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n",
    "\n",
    "    Dnm = np.max(np.abs(fpr - tpr))\n",
    "    return Dnm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __rolling_window(data, window_size):\n",
    "    \"\"\"\n",
    "    Rolling window: take window with definite size through the array\n",
    "\n",
    "    :param data: array-like\n",
    "    :param window_size: size\n",
    "    :return: the sequence of windows\n",
    "\n",
    "    Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4\n",
    "        Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n",
    "    \"\"\"\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "def __cvm(subindices, total_events):\n",
    "    \"\"\"\n",
    "    Compute Cramer-von Mises metric.\n",
    "    Compared two distributions, where first is subset of second one.\n",
    "    Assuming that second is ordered by ascending\n",
    "\n",
    "    :param subindices: indices of events which will be associated with the first distribution\n",
    "    :param total_events: count of events in the second distribution\n",
    "    :return: cvm metric\n",
    "    \"\"\"\n",
    "    target_distribution = np.arange(1, total_events + 1, dtype='float') / total_events\n",
    "    subarray_distribution = np.cumsum(np.bincount(subindices, minlength=total_events), dtype='float')\n",
    "    subarray_distribution /= 1.0 * subarray_distribution[-1]\n",
    "    return np.mean((target_distribution - subarray_distribution) ** 2)\n",
    "\n",
    "def compute_cvm(predictions, masses, n_neighbours=200, step=50):\n",
    "    \"\"\"\n",
    "    Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n",
    "    In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n",
    "\n",
    "    :param predictions: array-like, predictions\n",
    "    :param masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n",
    "    :param n_neighbours: count of neighbours for event to define mass bin\n",
    "    :param step: step through sorted mass-array to define next center of bin\n",
    "    :return: average cvm value\n",
    "    \"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    masses = np.array(masses)\n",
    "    assert len(predictions) == len(masses)\n",
    "\n",
    "    # First, reorder by masses\n",
    "    predictions = predictions[np.argsort(masses)]\n",
    "\n",
    "    # Second, replace probabilities with order of probability among other events\n",
    "    predictions = np.argsort(np.argsort(predictions, kind='mergesort'), kind='mergesort')\n",
    "\n",
    "    # Now, each window forms a group, and we can compute contribution of each group to CvM\n",
    "    cvms = []\n",
    "    for window in __rolling_window(predictions, window_size=n_neighbours)[::step]:\n",
    "        cvms.append(__cvm(subindices=window, total_events=len(predictions)))\n",
    "    return np.mean(cvms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.21436157140235346\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement.drop(['signal', 'id', 'weight'], axis = 1))[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0009934509874436174\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "correlation_probs = model.predict_proba(check_correlation.drop(['mass'], axis = 1))[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the agreement test fails with this model. So let us create new features and retrain our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'LifeTime', 'dira', 'FlightDistance', 'FlightDistanceError', 'IP',\n",
       "       'IPSig', 'VertexChi2', 'pt', 'DOCAone', 'DOCAtwo', 'DOCAthree',\n",
       "       'IP_p0p2', 'IP_p1p2', 'isolationa', 'isolationb', 'isolationc',\n",
       "       'isolationd', 'isolatione', 'isolationf', 'iso', 'CDF1', 'CDF2', 'CDF3',\n",
       "       'ISO_SumBDT', 'p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT', 'p0_track_Chi2Dof',\n",
       "       'p1_track_Chi2Dof', 'p2_track_Chi2Dof', 'p0_IP', 'p1_IP', 'p2_IP',\n",
       "       'p0_IPSig', 'p1_IPSig', 'p2_IPSig', 'p0_pt', 'p1_pt', 'p2_pt', 'p0_p',\n",
       "       'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta', 'SPDhits', 'production',\n",
       "       'signal', 'mass', 'min_ANNmuon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here are some features like 'isolationa', 'isolationb', 'isolationc' and 'p0_IP', 'p1_IP', 'p2_IP' and many others which by \n",
    "#their name sound like are related and represent same physical quantity in some way. So let us use such similar features to \n",
    "#create new features\n",
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 51)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 57)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us retrain the model with these new features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter...\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt',\n",
       "                                            'DOCAone', 'DOCAtwo', 'DOCAthree',\n",
       "                                            'IP_p0p2', 'IP_p1p2', 'isolationa',\n",
       "                                            'isolationb', 'isolationc',\n",
       "                                            'isolationd', 'isolatione',\n",
       "                                            'isolationf', 'iso', 'CDF1', 'CDF2',\n",
       "                                            'CDF3', 'ISO_SumBDT', 'p0_IsoBDT',\n",
       "                                            'p1_IsoBDT', 'p2_IsoBDT',\n",
       "                                            'p0_track_Chi2Dof',\n",
       "                                            'p1_track_Chi2Dof',\n",
       "                                            'p2_track_Chi2Dof', ...],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal']\n",
    "features = list(f for f in train_df.columns if f not in remove)\n",
    "print(\"Train a UGradientBoostingClassifier with new features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.23424866937161137\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.001074948150581736\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**let us try removing some unimportant features based on EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','p2_eta','p0_p','isolationd','isolationf','p2_IP','p2_IsoBDT',\n",
    "          'p0_eta','isolationa', 'isolationb', 'isolationc', 'isolationd','isolatione', 'isolationf','p1_p','p1_IsoBDT',\n",
    "          'CDF1','p0_pt','p1_pt', 'p2_pt','p1_IP','p2_p','FlightDistance','p1_track_Chi2Dof','p1_eta', 'p0_eta',\n",
    "          'FlightDistanceError','p0_IP','p1_IPSig','p2_track_Chi2Dof','pt', 'p0_IP', 'p1_IP', 'p2_IP', 'p0_p', 'p1_p',\n",
    "          'p2_p', 'p0_IPSig', 'p1_IPSig', 'p2_IPSig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(f for f in train_df.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features and removing some features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter='best', subsample=0.7,\n",
       "                            train_features=['LifeTime', 'dira', 'IP', 'IPSig',\n",
       "                                            'VertexChi2', 'DOCAone', 'DOCAtwo',\n",
       "                                            'DOCAthree', 'IP_p0p2', 'IP_p1p2',\n",
       "                                            'iso', 'CDF2', 'CDF3', 'ISO_SumBDT',\n",
       "                                            'p0_IsoBDT', 'p0_track_Chi2Dof',\n",
       "                                            'SPDhits'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 23)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1[features + ['mass']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_df_1['signal'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(train_df_1['signal']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.22324408272050666\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0010256531459170882\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the agreement test is still failing. Let us add some more features from lierature where similar problem has been solved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDu=istance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','p2_eta','p0_p','isolationd','isolationf','p2_IP','p2_IsoBDT',\n",
    "          'p0_eta','isolationa', 'isolationb', 'isolationc', 'isolationd','isolatione', 'isolationf','p1_p','p1_IsoBDT',\n",
    "          'CDF1','p0_pt','p1_pt', 'p2_pt','p1_IP','p2_p','FlightDistance','p1_track_Chi2Dof','p1_eta', 'p0_eta',\n",
    "          'FlightDistanceError','p0_IP','p1_IPSig','p2_track_Chi2Dof','pt', 'p0_IP', 'p1_IP', 'p2_IP', 'p0_p', 'p1_p',\n",
    "          'p2_p', 'p0_IPSig', 'p1_IPSig', 'p2_IPSig']\n",
    "\n",
    "features = list(f for f in train_df.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features and removing some features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter='best', subsample=0.7,\n",
       "                            train_features=['LifeTime', 'dira', 'IP', 'IPSig',\n",
       "                                            'VertexChi2', 'DOCAone', 'DOCAtwo',\n",
       "                                            'DOCAthree', 'IP_p0p2', 'IP_p1p2',\n",
       "                                            'iso', 'CDF2', 'CDF3', 'ISO_SumBDT',\n",
       "                                            'p0_IsoBDT', 'p0_track_Chi2Dof',\n",
       "                                            'SPDhits'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.2191779035101546\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.001090672320032115\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the agreement test is still failing. Let us try adding some more features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feats(df):\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDu=istance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67553, 61)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping some more unimportant features\n",
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','SPDhits','CDF1', 'CDF2', 'CDF3','isolationb', 'isolationc',\n",
    "          'p0_pt', 'p1_pt', 'p2_pt','p0_p', 'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta','isolationa', 'isolationb', \n",
    "          'isolationc','isolationd', 'isolatione', 'isolationf','p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT','p0_IP', 'p1_IP',\n",
    "          'p2_IP','IP_p0p2','IP_p1p2','p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof','p0_IPSig', 'p1_IPSig',\n",
    "          'p2_IPSig','DOCAone','DOCAtwo', 'DOCAthree']\n",
    "features = list(f for f in train_df.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'LifeTime', 'dira', 'FlightDistance', 'FlightDistanceError', 'IP',\n",
       "       'IPSig', 'VertexChi2', 'pt', 'DOCAone', 'DOCAtwo', 'DOCAthree',\n",
       "       'IP_p0p2', 'IP_p1p2', 'isolationa', 'isolationb', 'isolationc',\n",
       "       'isolationd', 'isolatione', 'isolationf', 'iso', 'CDF1', 'CDF2', 'CDF3',\n",
       "       'ISO_SumBDT', 'p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT', 'p0_track_Chi2Dof',\n",
       "       'p1_track_Chi2Dof', 'p2_track_Chi2Dof', 'p0_IP', 'p1_IP', 'p2_IP',\n",
       "       'p0_IPSig', 'p1_IPSig', 'p2_IPSig', 'p0_pt', 'p1_pt', 'p2_pt', 'p0_p',\n",
       "       'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta', 'SPDhits', 'production',\n",
       "       'signal', 'mass', 'min_ANNmuon', 'isolation_abc', 'isolation_def',\n",
       "       'p_IP', 'p_p', 'IP_pp', 'p_IPSig', 'FD_LT', 'FD_p0p1p2_p', 'NEW5_lt',\n",
       "       'Chi2Dof_MAX'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features and removing some features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter='best', subsample=0.7,\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LifeTime', 'dira', 'FlightDistance', 'FlightDistanceError', 'IP',\n",
       "       'IPSig', 'VertexChi2', 'pt', 'iso', 'ISO_SumBDT', 'mass'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1[features + ['mass']].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.04482301271830369\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0009370011153217489\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(test_df[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"result3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adding some more features from kaggle discussion forums to enhance the performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDu=istance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    # features from kaggle discussion forum\n",
    "    df2['flight_dist_sig2'] = (df['FlightDistance']/df['FlightDistanceError'])**2\n",
    "    df2['flight_dist_sig'] = df['FlightDistance']/df['FlightDistanceError']\n",
    "    df2['NEW_IP_dira'] = df['IP']*df['dira']\n",
    "    df2['p0p2_ip_ratio']=df['IP']/df['IP_p0p2']\n",
    "    df2['p1p2_ip_ratio']=df['IP']/df['IP_p1p2']\n",
    "    df2['DCA_MAX'] = df.loc[:, ['DOCAone', 'DOCAtwo', 'DOCAthree']].max(axis=1)\n",
    "    df2['iso_bdt_min'] = df.loc[:, ['p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT']].min(axis=1)\n",
    "    df2['iso_min'] = df.loc[:, ['isolationa', 'isolationb', 'isolationc','isolationd', 'isolatione', 'isolationf']].min(axis=1)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','SPDhits','CDF1', 'CDF2', 'CDF3','isolationb', 'isolationc',\n",
    "          'p0_pt', 'p1_pt', 'p2_pt','p0_p', 'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta','isolationa', 'isolationb',\n",
    "          'isolationc', 'isolationd', 'isolatione', 'isolationf','p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT','p0_IP', 'p1_IP',\n",
    "          'p2_IP','IP_p0p2', 'IP_p1p2','p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof','p0_IPSig', 'p1_IPSig',\n",
    "          'p2_IPSig','DOCAone', 'DOCAtwo', 'DOCAthree']\n",
    "features = list(f for f in train_df_1.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features and removing some features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter...\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT', 'isolation_abc',\n",
       "                                            'isolation_def', 'p_IP', 'p_p',\n",
       "                                            'IP_pp', 'p_IPSig', 'FD_LT',\n",
       "                                            'FD_p0p1p2_p', 'NEW5_lt',\n",
       "                                            'Chi2Dof_MAX', 'flight_dist_sig2',\n",
       "                                            'flight_dist_sig', 'NEW_IP_dira',\n",
       "                                            'p0p2_ip_ratio', 'p1p2_ip_ratio',\n",
       "                                            'DCA_MAX', 'iso_bdt_min',\n",
       "                                            'iso_min'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.07795205982595821\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0017555368714286441\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(test_df_1[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"result4a.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both tests pass and performance of this model with test data on kaggle is also improved. Let us try using a different algorithm of RandomForestClassifier with hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True],\n",
       "                                        'max_depth': [5, 10, 20, 40, 80],\n",
       "                                        'max_features': ['auto'],\n",
       "                                        'min_samples_leaf': [2, 4, 10],\n",
       "                                        'min_samples_split': [5, 10, 20, 40],\n",
       "                                        'n_estimators': [100, 200, 500, 600,\n",
       "                                                         800]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100,200,500,600,800]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5,10,20,40,80]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10, 20, 40]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 4, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_df_1[features], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=40, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=40, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=40, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=2, min_samples_split=5,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "model.fit(train_df_1[features], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.04498071333281184\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0009992148828636693\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(test_df_1[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"result5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both the tests pass with RandomForest model but performance is not as greate as UGradientBoosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us try different variations of feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying variations in operations to create some new features based on kaggle discussion forum\n",
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] * df['isolationb'] * df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] * df['isolatione'] * df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDu=istance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    # features from kaggle discussion forum\n",
    "    df2['flight_dist_sig2'] = (df['FlightDistance']/df['FlightDistanceError'])**2\n",
    "    df2['flight_dist_sig'] = df['FlightDistance']/df['FlightDistanceError']\n",
    "    df2['NEW_IP_dira'] = df['IP']*df['dira']\n",
    "    df2['p0p2_ip_ratio']=df['IP']/df['IP_p0p2']\n",
    "    df2['p1p2_ip_ratio']=df['IP']/df['IP_p1p2']\n",
    "    df2['DCA_MAX'] = df.loc[:, ['DOCAone', 'DOCAtwo', 'DOCAthree']].max(axis=1)\n",
    "    df2['iso_bdt_min'] = df.loc[:, ['p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT']].min(axis=1)\n",
    "    df2['iso_min'] = df.loc[:, ['isolationa', 'isolationb', 'isolationc','isolationd', 'isolatione', 'isolationf']].min(axis=1)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal',\n",
    "              'SPDhits','CDF1', 'CDF2', 'CDF3',\n",
    "              'isolationb', 'isolationc','p0_pt', 'p1_pt', 'p2_pt',\n",
    "              'p0_p', 'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta',\n",
    "              'isolationa', 'isolationb', 'isolationc', 'isolationd', 'isolatione', 'isolationf',\n",
    "              'p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT',\n",
    "              'p0_IP', 'p1_IP', 'p2_IP',\n",
    "              'IP_p0p2', 'IP_p1p2',\n",
    "              'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof',\n",
    "              'p0_IPSig', 'p1_IPSig', 'p2_IPSig',\n",
    "              'DOCAone', 'DOCAtwo', 'DOCAthree']\n",
    "features = list(f for f in train_df.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a UGradientBoostingClassifier with new features and removing some features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitter='best', subsample=0.7,\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth=6,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.04186936644380568\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0009566844499362014\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(test_df_1[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"result9.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model with slight changes in features passes the tests but does not perform as good as previous model. So sticking with the UGraientBoosting algorithm itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.15,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=7, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=1000,\n",
       "                            random_state=RandomState(MT19937) at 0x1AEE92DC940,\n",
       "                            splitte...\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT', 'isolation_abc',\n",
       "                                            'isolation_def', 'p_IP', 'p_p',\n",
       "                                            'IP_pp', 'p_IPSig', 'FD_LT',\n",
       "                                            'FD_p0p1p2_p', 'NEW5_lt',\n",
       "                                            'Chi2Dof_MAX', 'flight_dist_sig2',\n",
       "                                            'flight_dist_sig', 'NEW_IP_dira',\n",
       "                                            'p0p2_ip_ratio', 'p1p2_ip_ratio',\n",
       "                                            'DCA_MAX', 'iso_bdt_min',\n",
       "                                            'iso_min'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDu=istance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    # features from kaggle discussion forum\n",
    "    df2['flight_dist_sig2'] = (df['FlightDistance']/df['FlightDistanceError'])**2\n",
    "    df2['flight_dist_sig'] = df['FlightDistance']/df['FlightDistanceError']\n",
    "    df2['NEW_IP_dira'] = df['IP']*df['dira']\n",
    "    df2['p0p2_ip_ratio']=df['IP']/df['IP_p0p2']\n",
    "    df2['p1p2_ip_ratio']=df['IP']/df['IP_p1p2']\n",
    "    df2['DCA_MAX'] = df.loc[:, ['DOCAone', 'DOCAtwo', 'DOCAthree']].max(axis=1)\n",
    "    df2['iso_bdt_min'] = df.loc[:, ['p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT']].min(axis=1)\n",
    "    df2['iso_min'] = df.loc[:, ['isolationa', 'isolationb', 'isolationc','isolationd', 'isolatione', 'isolationf']].min(axis=1)\n",
    "    return df2\n",
    "\n",
    "\n",
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)\n",
    "\n",
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','SPDhits','CDF1', 'CDF2', 'CDF3','isolationb', 'isolationc',\n",
    "          'p0_pt', 'p1_pt', 'p2_pt','p0_p', 'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta','isolationa', 'isolationb',\n",
    "          'isolationc', 'isolationd', 'isolatione', 'isolationf','p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT','p0_IP', 'p1_IP',\n",
    "          'p2_IP','IP_p0p2', 'IP_p1p2','p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof','p0_IPSig', 'p1_IPSig',\n",
    "          'p2_IPSig','DOCAone', 'DOCAtwo', 'DOCAthree']\n",
    "features = list(f for f in train_df_1.columns if f not in remove)\n",
    "\n",
    "#len(features)\n",
    "\n",
    "#print(\"Train a UGradientBoostingClassifier with new features and removing some features\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=1000,\n",
    "                                 max_depth=7,\n",
    "                                 learning_rate=0.15,\n",
    "                                 train_features=features,\n",
    "                                 subsample=0.7)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.07434361234769682\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.0021117647116199053\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(test_df_1[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"result9.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
