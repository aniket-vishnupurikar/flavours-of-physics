{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Loading datasets\n",
    "train_df = pd.read_csv(\"training.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "agree_df = pd.read_csv(\"check_agreement.csv\")\n",
    "corr_df = pd.read_csv(\"check_correlation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing functions for agreement and correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check agrrement test\n",
    "\n",
    "\n",
    "# agreement test as mentioned in kaggle resources#\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n",
    "    \"\"\"\n",
    "    Compute roc curve\n",
    "\n",
    "    :param data_zero: 0-labeled data\n",
    "    :param data_one:  1-labeled data\n",
    "    :param sample_weights_zero: weights for 0-labeled data\n",
    "    :param sample_weights_one:  weights for 1-labeled data\n",
    "    :return: roc curve\n",
    "    \"\"\"\n",
    "    labels = [0] * len(data_zero) + [1] * len(data_one)\n",
    "    weights = np.concatenate([sample_weights_zero, sample_weights_one])\n",
    "    data_all = np.concatenate([data_zero, data_one])\n",
    "    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n",
    "    return fpr, tpr\n",
    "\n",
    "def compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n",
    "    \"\"\"\n",
    "    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n",
    "\n",
    "    :param data_prediction: array-like, real data predictions\n",
    "    :param mc_prediction: array-like, Monte Carlo data predictions\n",
    "    :param weights_data: array-like, real data weights\n",
    "    :param weights_mc: array-like, Monte Carlo weights\n",
    "    :return: ks value\n",
    "    \"\"\"\n",
    "    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n",
    "    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n",
    "\n",
    "    data_prediction, mc_prediction = np.array(data_prediction), np.array(mc_prediction)\n",
    "    weights_data, weights_mc = np.array(weights_data), np.array(weights_mc)\n",
    "\n",
    "    assert np.all(data_prediction >= 0.) and np.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n",
    "    assert np.all(mc_prediction >= 0.) and np.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n",
    "\n",
    "    weights_data /= np.sum(weights_data)\n",
    "    weights_mc /= np.sum(weights_mc)\n",
    "\n",
    "    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n",
    "\n",
    "    Dnm = np.max(np.abs(fpr - tpr))\n",
    "    return Dnm\n",
    "\n",
    "# check correlation test\n",
    "\n",
    "# correlation test as mentioned in kaggle resources\n",
    "\n",
    "def __rolling_window(data, window_size):\n",
    "    \"\"\"\n",
    "    Rolling window: take window with definite size through the array\n",
    "\n",
    "    :param data: array-like\n",
    "    :param window_size: size\n",
    "    :return: the sequence of windows\n",
    "\n",
    "    Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4\n",
    "        Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n",
    "    \"\"\"\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "def __cvm(subindices, total_events):\n",
    "    \"\"\"\n",
    "    Compute Cramer-von Mises metric.\n",
    "    Compared two distributions, where first is subset of second one.\n",
    "    Assuming that second is ordered by ascending\n",
    "\n",
    "    :param subindices: indices of events which will be associated with the first distribution\n",
    "    :param total_events: count of events in the second distribution\n",
    "    :return: cvm metric\n",
    "    \"\"\"\n",
    "    target_distribution = np.arange(1, total_events + 1, dtype='float') / total_events\n",
    "    subarray_distribution = np.cumsum(np.bincount(subindices, minlength=total_events), dtype='float')\n",
    "    subarray_distribution /= 1.0 * subarray_distribution[-1]\n",
    "    return np.mean((target_distribution - subarray_distribution) ** 2)\n",
    "\n",
    "def compute_cvm(predictions, masses, n_neighbours=200, step=50):\n",
    "    \"\"\"\n",
    "    Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n",
    "    In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n",
    "\n",
    "    :param predictions: array-like, predictions\n",
    "    :param masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n",
    "    :param n_neighbours: count of neighbours for event to define mass bin\n",
    "    :param step: step through sorted mass-array to define next center of bin\n",
    "    :return: average cvm value\n",
    "    \"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    masses = np.array(masses)\n",
    "    assert len(predictions) == len(masses)\n",
    "\n",
    "    # First, reorder by masses\n",
    "    predictions = predictions[np.argsort(masses)]\n",
    "\n",
    "    # Second, replace probabilities with order of probability among other events\n",
    "    predictions = np.argsort(np.argsort(predictions, kind='mergesort'), kind='mergesort')\n",
    "\n",
    "    # Now, each window forms a group, and we can compute contribution of each group to CvM\n",
    "    cvms = []\n",
    "    for window in __rolling_window(predictions, window_size=n_neighbours)[::step]:\n",
    "        cvms.append(__cvm(subindices=window, total_events=len(predictions)))\n",
    "    return np.mean(cvms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "def new_feats(df):\n",
    "    df2 = df.copy()\n",
    "    df2['isolation_abc'] = df['isolationa'] + df['isolationb'] + df['isolationc']\n",
    "    df2['isolation_def'] = df['isolationd'] + df['isolatione'] + df['isolationf']\n",
    "    df2['p_IP'] = df['p0_IP']+df['p1_IP']+df['p2_IP']\n",
    "    df2['p_p']  = df['p0_p']+df['p1_p']+df['p2_p']\n",
    "    df2['IP_pp'] = df['IP_p0p2'] + df['IP_p1p2']\n",
    "    df2['p_IPSig'] = df['p0_IPSig'] + df['p1_IPSig'] + df['p2_IPSig']\n",
    "    #new feature using 'FlightDistance' and LifeTime(from literature)\n",
    "    df2['FD_LT']=df['FlightDistance']/df['LifeTime']\n",
    "    #new feature using 'FlightDistance', 'po_p', 'p1_p', 'p2_p'(from literature)\n",
    "    df2['FD_p0p1p2_p']=df['FlightDistance']/(df['p0_p']+df['p1_p']+df['p2_p'])\n",
    "    #new feature using 'LifeTime', 'p0_IP', 'p1_IP', 'p2_IP'(from literature)\n",
    "    df2['NEW5_lt']=df['LifeTime']*(df['p0_IP']+df['p1_IP']+df['p2_IP'])/3\n",
    "    #new feature using 'p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof'(taking max value among 3 features for each row)\n",
    "    df2['Chi2Dof_MAX'] = df.loc[:, ['p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof']].max(axis=1)\n",
    "    # features from kaggle discussion forum\n",
    "    df2['flight_dist_sig2'] = (df['FlightDistance']/df['FlightDistanceError'])**2\n",
    "    df2['flight_dist_sig'] = df['FlightDistance']/df['FlightDistanceError']\n",
    "    df2['NEW_IP_dira'] = df['IP']*df['dira']\n",
    "    df2['p0p2_ip_ratio']=df['IP']/df['IP_p0p2']\n",
    "    df2['p1p2_ip_ratio']=df['IP']/df['IP_p1p2']\n",
    "    df2['DCA_MAX'] = df.loc[:, ['DOCAone', 'DOCAtwo', 'DOCAthree']].max(axis=1)\n",
    "    df2['iso_bdt_min'] = df.loc[:, ['p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT']].min(axis=1)\n",
    "    df2['iso_min'] = df.loc[:, ['isolationa', 'isolationb', 'isolationc','isolationd', 'isolatione', 'isolationf']].min(axis=1)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding engineered features to training and test datasets\n",
    "train_df_1 = new_feats(train_df)\n",
    "test_df_1 = new_feats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idenifying some features to remove which have been used to engineer new features and which have been observed to be of \n",
    "#low importance in EDA\n",
    "remove = ['id', 'min_ANNmuon', 'production', 'mass', 'signal','SPDhits','CDF1', 'CDF2', 'CDF3','isolationb', 'isolationc',\n",
    "          'p0_pt', 'p1_pt', 'p2_pt','p0_p', 'p1_p', 'p2_p', 'p0_eta', 'p1_eta', 'p2_eta','isolationa', 'isolationb',\n",
    "          'isolationc', 'isolationd', 'isolatione', 'isolationf','p0_IsoBDT', 'p1_IsoBDT', 'p2_IsoBDT','p0_IP', 'p1_IP',\n",
    "          'p2_IP','IP_p0p2', 'IP_p1p2','p0_track_Chi2Dof', 'p1_track_Chi2Dof', 'p2_track_Chi2Dof','p0_IPSig', 'p1_IPSig',\n",
    "          'p2_IPSig','DOCAone', 'DOCAtwo', 'DOCAthree']\n",
    "# making a list of features to be used to train the model and make predictions\n",
    "features = list(f for f in train_df_1.columns if f not in remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new class for UGradientBoosting with loss incorporated in the class itself for bayesian optimization hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "from hep_ml.losses import BinFlatnessLossFunction\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "class UGradientBoostingClassifierWithLoss(BaseEstimator):\n",
    "    def __init__(\n",
    "#         self, max_depth=3, max_features=0.8, learning_rate=0.01,\n",
    "#         n_estimators=80, subsample=0.8\n",
    "        self, max_depth, n_estimators, **params\n",
    "    ):\n",
    "        loss = BinFlatnessLossFunction(\n",
    "            ['mass'], n_bins=15, uniform_label = 0 , fl_coefficient=15, power=2\n",
    "        )\n",
    "        \n",
    "        self.estimator = UGradientBoostingClassifier(\n",
    "            loss=loss,\n",
    "            train_features = list(f for f in train_df_1.columns if f not in remove),\n",
    "            max_depth = max_depth,\n",
    "            n_estimators = n_estimators,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.estimator.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.estimator.transform(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        params_to_keep = [        \n",
    "             \"max_depth\",\n",
    "            \"max_features\",\n",
    "            \"learning_rate\",\n",
    "             \"n_estimators\",\n",
    "            \"subsample\",\n",
    "        ]\n",
    "        \n",
    "        ret = dict()\n",
    "        tret = self.estimator.get_params(deep=deep)\n",
    "        for key in params_to_keep:\n",
    "            ret[key] = tret[key]\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        self.estimator.get_params(parameters)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.estimator.predict(X)\n",
    "        \n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        print(acc)\n",
    "        print(Counter(y))\n",
    "        print(Counter(y_pred))\n",
    "        \n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "from hep_ml.losses import BinFlatnessLossFunction\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, accuracy_score\n",
    "import time\n",
    "import json\n",
    "auc = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bayesian optimization demands bounds of hyperparameters. Therefore parameters with discrete values can not be tuned with this technique. The discrete hyperparameters neede to be tuned are n_estimators and max_depth. Therefore creating all possible pairs of these two parameters and doing grid search for these two parameters while running bayesian optimization for each pair of these two discrete parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth-n_estimator pair: (3, 50)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8579  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8556  \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 0.6477  \u001b[0m | \u001b[0m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8399  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8575  \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 0.9954  \u001b[0m | \u001b[0m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8471  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8558  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8466  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.8592  \u001b[0m | \u001b[95m 0.8422  \u001b[0m | \u001b[95m 0.9076  \u001b[0m | \u001b[95m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8564  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8409  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8547  \u001b[0m | \u001b[0m 0.9651  \u001b[0m | \u001b[0m 0.5051  \u001b[0m | \u001b[0m 0.9868  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8567  \u001b[0m | \u001b[0m 0.4068  \u001b[0m | \u001b[0m 0.5035  \u001b[0m | \u001b[0m 0.9996  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.858   \u001b[0m | \u001b[0m 0.5348  \u001b[0m | \u001b[0m 0.9845  \u001b[0m | \u001b[0m 0.9997  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (3, 400)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8722  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8739  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8559  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8712  \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 0.9954  \u001b[0m | \u001b[0m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8625  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8698  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8629  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8714  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8739  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8572  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8724  \u001b[0m | \u001b[0m 0.9963  \u001b[0m | \u001b[0m 0.9844  \u001b[0m | \u001b[0m 0.5048  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8705  \u001b[0m | \u001b[0m 0.9954  \u001b[0m | \u001b[0m 0.5112  \u001b[0m | \u001b[0m 0.5096  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8715  \u001b[0m | \u001b[0m 0.608   \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.507   \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (3, 900)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.88    \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8822  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8619  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 0.9954  \u001b[0m | \u001b[0m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8682  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8764  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8668  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8782  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8801  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8631  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 0.9963  \u001b[0m | \u001b[0m 0.9844  \u001b[0m | \u001b[0m 0.5048  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8805  \u001b[0m | \u001b[0m 0.9904  \u001b[0m | \u001b[0m 0.518   \u001b[0m | \u001b[0m 0.9965  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8794  \u001b[0m | \u001b[0m 0.9681  \u001b[0m | \u001b[0m 0.5242  \u001b[0m | \u001b[0m 0.5118  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (6, 50)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8557  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8568  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8527  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8624  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8574  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8615  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8569  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8521  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8598  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8528  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8581  \u001b[0m | \u001b[0m 0.6638  \u001b[0m | \u001b[0m 0.9997  \u001b[0m | \u001b[0m 0.5163  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8603  \u001b[0m | \u001b[0m 0.4139  \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 0.996   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.8629  \u001b[0m | \u001b[95m 0.3242  \u001b[0m | \u001b[95m 0.9863  \u001b[0m | \u001b[95m 0.5006  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (6, 400)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.872   \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8729  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.865   \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8798  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8712  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8764  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8698  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8692  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8745  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8651  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.873   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8604  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8795  \u001b[0m | \u001b[0m 0.4756  \u001b[0m | \u001b[0m 0.9829  \u001b[0m | \u001b[0m 0.5015  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (6, 900)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.881   \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8698  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8866  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8771  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8839  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8764  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8735  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8822  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8707  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 0.3301  \u001b[0m | \u001b[0m 0.518   \u001b[0m | \u001b[0m 0.9939  \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.8887  \u001b[0m | \u001b[95m 0.4592  \u001b[0m | \u001b[95m 0.9965  \u001b[0m | \u001b[95m 0.5016  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8867  \u001b[0m | \u001b[0m 0.3242  \u001b[0m | \u001b[0m 0.9863  \u001b[0m | \u001b[0m 0.5006  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (9, 50)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8488  \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 0.6477  \u001b[0m | \u001b[0m 0.5746  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8575  \u001b[0m | \u001b[95m 0.03225 \u001b[0m | \u001b[95m 0.7101  \u001b[0m | \u001b[95m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8584  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8591  \u001b[0m | \u001b[95m 0.09038 \u001b[0m | \u001b[95m 0.8348  \u001b[0m | \u001b[95m 0.8106  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.8615  \u001b[0m | \u001b[95m 0.2815  \u001b[0m | \u001b[95m 0.7331  \u001b[0m | \u001b[95m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8604  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8459  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8539  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8567  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.855   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8596  \u001b[0m | \u001b[0m 0.1928  \u001b[0m | \u001b[0m 0.9686  \u001b[0m | \u001b[0m 0.9964  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8594  \u001b[0m | \u001b[0m 0.08616 \u001b[0m | \u001b[0m 0.9998  \u001b[0m | \u001b[0m 0.5091  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (9, 400)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8684  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8689  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8664  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8758  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8724  \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8752  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8646  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8717  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8677  \u001b[0m | \u001b[0m 0.0964  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.8763  \u001b[0m | \u001b[95m 0.383   \u001b[0m | \u001b[95m 0.9952  \u001b[0m | \u001b[95m 0.5011  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 0.3571  \u001b[0m | \u001b[0m 0.9953  \u001b[0m | \u001b[0m 0.9967  \u001b[0m |\n",
      "=============================================================\n",
      "max_depth-n_estimator pair: (9, 900)\n",
      "|   iter    |  target   | learni... | max_fe... | subsample |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8736  \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.5845  \u001b[0m | \u001b[0m 0.718   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8736  \u001b[0m | \u001b[95m 0.7716  \u001b[0m | \u001b[95m 0.6477  \u001b[0m | \u001b[95m 0.5746  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8706  \u001b[0m | \u001b[0m 0.03225 \u001b[0m | \u001b[0m 0.7101  \u001b[0m | \u001b[0m 0.6193  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8811  \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9954  \u001b[0m | \u001b[95m 0.6189  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.876   \u001b[0m | \u001b[0m 0.09038 \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8793  \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 0.7331  \u001b[0m | \u001b[0m 0.5592  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8758  \u001b[0m | \u001b[0m 0.08322 \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.897   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8681  \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.9955  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 0.5815  \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.7107  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8722  \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.5527  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.8818  \u001b[0m | \u001b[95m 0.4855  \u001b[0m | \u001b[95m 0.9904  \u001b[0m | \u001b[95m 0.504   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.877   \u001b[0m | \u001b[0m 0.0682  \u001b[0m | \u001b[0m 0.999   \u001b[0m | \u001b[0m 0.5176  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8817  \u001b[0m | \u001b[0m 0.4756  \u001b[0m | \u001b[0m 0.9829  \u001b[0m | \u001b[0m 0.5015  \u001b[0m |\n",
      "=============================================================\n",
      "It takes 1945.4509294231732 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# we will save best parameters in best dictionary\n",
    "best = dict()\n",
    "best['score'] = 0\n",
    "import itertools\n",
    "max_depth = [3,6,9]\n",
    "n_estimators = [50,400,900]\n",
    "for x in itertools.product(max_depth, n_estimators):\n",
    "    \n",
    "    print(\"max_depth-n_estimator pair: {}\".format(x))\n",
    "    def gbm_cl_bo(max_features, learning_rate, subsample):\n",
    "        \n",
    "        params_gbm = {}\n",
    "    #   params_gbm['max_depth'] = round(max_depth)\n",
    "        params_gbm['max_features'] = max_features\n",
    "        params_gbm['learning_rate'] = learning_rate\n",
    "    #   params_gbm['n_estimators'] = round(n_estimators)\n",
    "        params_gbm['subsample'] = subsample\n",
    "    \n",
    "        \n",
    "    #     loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label = 0 , fl_coefficient=15, power=2)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        skf.get_n_splits(train_df_1[features + ['mass']], train_df_1['signal'])\n",
    "\n",
    "        scores = cross_val_score(UGradientBoostingClassifierWithLoss(max_depth = x[0], n_estimators = x[1], **params_gbm),\n",
    "                                 train_df_1[features + ['mass']], train_df_1['signal'],\n",
    "                                 scoring = auc,\n",
    "                                 cv=skf)\n",
    "        score = scores.mean()\n",
    "        #print(\"max_depth: {}, n_estimators: {}, max_features: {}, learning_rate: {}, subsample: {}-------score: {}\"\n",
    "        #      .format(x[0], x[1], params_gbm['max_features'], params_gbm['learning_rate'], params_gbm['subsample'], score))\n",
    "\n",
    "        if score > best['score']:\n",
    "            best['score'] = score\n",
    "            best['max_depth'] = x[0]\n",
    "            best['n_estimators'] = x[1]\n",
    "            best['max_features'] = params_gbm['max_features']\n",
    "            best['learning_rate'] = params_gbm['learning_rate']\n",
    "            best['subsample'] = params_gbm['subsample']\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    params_gbm ={\n",
    "#    'max_depth': (3),\n",
    "    'max_features':(0.5, 1),\n",
    "    'learning_rate':(0.01, 1),\n",
    "#    'n_estimators': (100),\n",
    "    'subsample': (0.5, 1)\n",
    "    }\n",
    "    \n",
    "    gbm_bo = BayesianOptimization(gbm_cl_bo, params_gbm, random_state=111)\n",
    "    gbm_bo.maximize(init_points=10, n_iter=3)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "print('It takes %s minutes' % ((time.time() - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**took nearly 35 hours to run!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth-n_estimator pair: (6, 900)\n",
    "\n",
    "iter    |  target   | learni... | max_fe... | subsample |\n",
    "\n",
    "12      |  0.8887   |  0.4592   |  0.9965   |  0.5016   |\n",
    "\n",
    "4        |  0.8866   |  0.3443   |  0.9954   |  0.6189 \n",
    "\n",
    "13       |  0.8867   |  0.3242   |  0.9863   |  0.5006\n",
    "\n",
    "2        |  0.881    |  0.7716   |  0.6477   |  0.5746 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8886783798768416,\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 900,\n",
       " 'max_features': 0.9965346255608354,\n",
       " 'learning_rate': 0.459200553861577,\n",
       " 'subsample': 0.501638892280019}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**it is found that the hyperparameter which gives best score does not pass correlation test. Therefore trying other hyperparameters with best scores in descending order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "UGradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**following model with passed parameters passes both the required tests. Therefore using the results from this model to check on kaggle test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.1,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x290F6367740,\n",
       "                            splitter=...\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT', 'isolation_abc',\n",
       "                                            'isolation_def', 'p_IP', 'p_p',\n",
       "                                            'IP_pp', 'p_IPSig', 'FD_LT',\n",
       "                                            'FD_p0p1p2_p', 'NEW5_lt',\n",
       "                                            'Chi2Dof_MAX', 'flight_dist_sig2',\n",
       "                                            'flight_dist_sig', 'NEW_IP_dira',\n",
       "                                            'p0p2_ip_ratio', 'p1p2_ip_ratio',\n",
       "                                            'DCA_MAX', 'iso_bdt_min',\n",
       "                                            'iso_min'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0 , fl_coefficient=15, power=2)\n",
    "model = UGradientBoostingClassifier(loss=loss, n_estimators=900,\n",
    "                                 max_depth = 6,\n",
    "                                 learning_rate = 0.1,\n",
    "                                 train_features = features,\n",
    "                                 subsample=0.5)\n",
    "model.fit(train_df_1[features + ['mass']], train_df_1['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to the memory\n",
    "import pickle\n",
    "filename = 'finalized_model_1.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model from memory\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UGradientBoostingClassifier(learning_rate=0.1,\n",
       "                            loss=BinFlatnessLossFunction(allow_wrong_signs=True,\n",
       "                                                         fl_coefficient=15,\n",
       "                                                         n_bins=15, power=2,\n",
       "                                                         uniform_features=['mass'],\n",
       "                                                         uniform_label=array([0])),\n",
       "                            max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            n_estimators=900,\n",
       "                            random_state=RandomState(MT19937) at 0x290A3B58140,\n",
       "                            splitter=...\n",
       "                            train_features=['LifeTime', 'dira',\n",
       "                                            'FlightDistance',\n",
       "                                            'FlightDistanceError', 'IP',\n",
       "                                            'IPSig', 'VertexChi2', 'pt', 'iso',\n",
       "                                            'ISO_SumBDT', 'isolation_abc',\n",
       "                                            'isolation_def', 'p_IP', 'p_p',\n",
       "                                            'IP_pp', 'p_IPSig', 'FD_LT',\n",
       "                                            'FD_p0p1p2_p', 'NEW5_lt',\n",
       "                                            'Chi2Dof_MAX', 'flight_dist_sig2',\n",
       "                                            'flight_dist_sig', 'NEW_IP_dira',\n",
       "                                            'p0p2_ip_ratio', 'p1p2_ip_ratio',\n",
       "                                            'DCA_MAX', 'iso_bdt_min',\n",
       "                                            'iso_min'],\n",
       "                            update_tree=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS metric 0.0795798778412874\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# conducting agreement check test\n",
    "check_agreement = pd.read_csv(\"check_agreement.csv\")\n",
    "check_agreement = new_feats(check_agreement)\n",
    "\n",
    "#check_agreement = pandas.read_csv(folder + 'check_agreement.csv', index_col='id')\n",
    "agreement_probs = model.predict_proba(check_agreement[features])[:, 1]\n",
    "\n",
    "ks = compute_ks(\n",
    "    agreement_probs[check_agreement['signal'].values == 0],\n",
    "    agreement_probs[check_agreement['signal'].values == 1],\n",
    "    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "#print 'KS metric', ks, ks < 0.09\n",
    "print(\"KS metric {}\".format(ks))\n",
    "print(ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvM metric 0.00135303046269074\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# conducting the correlation test\n",
    "#check_correlation = pandas.read_csv(folder + 'check_correlation.csv', index_col='id')\n",
    "check_correlation = pd.read_csv(\"check_correlation.csv\", index_col = \"id\")\n",
    "check_correlation = new_feats(check_correlation)\n",
    "correlation_probs = model.predict_proba(check_correlation[features])[:, 1]\n",
    "cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "#print 'CvM metric', cvm, cvm < 0.002\n",
    "print(\"CvM metric {}\".format(cvm))\n",
    "print(cvm < 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making submission file with test dataset to be submitted on kaggle\n",
    "test_probs = model.predict_proba(test_df_1[features])[:,1]\n",
    "result = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_probs})\n",
    "result.to_csv(\"final_result_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20th rank with final_result_1.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
